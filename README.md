# ğŸ¤– Talk Fusion â€” Full-Stack AI Chat Application

**Talk Fusion** is a modern **full-stack AI chat application** built with **React (frontend)** and **Express.js (backend)**, powered by **Groqâ€™s LLaMA 3.1** model.
It supports **context-aware conversations**, **Markdown-formatted responses**, and a **clean, responsive UI**.

---

## ğŸš€ Features

* ğŸ’¬ **Real-time AI Chat Interface**
* ğŸ§  **Conversation Memory (Multi-turn Context)**
* âœï¸ **Markdown Rendering** (headings, lists, code blocks)
* âš¡ **Fast & Responsive UI**
* ğŸ”— **REST-based Frontendâ€“Backend Integration**
* ğŸŒ **CORS-enabled Express Backend**
* ğŸ§© **Intent-based Prompt Handling** (code vs explanation)

---

## ğŸ› ï¸ Tech Stack

### Frontend

* React
* JavaScript (ES6+)
* CSS / Tailwind CSS
* React Hooks (`useState`, `useEffect`)
* Axios (API communication)

### Backend

* Node.js
* Express.js
* Groq SDK (LLaMA 3.1)
* REST API
* dotenv (environment variables)
* CORS middleware

---

## ğŸ”— Backend API Contract

### POST `/chat`

**Request Body**

```json
{
  "message": "Your message here",
  "history": [
    { "role": "user", "content": "Hello" },
    { "role": "assistant", "content": "Hi!" }
  ]
}
```

**Response**

```json
{
  "reply": "AI-generated response"
}
```

---

## ğŸ§  How Conversation Memory Works

* The **frontend maintains full chat history**
* Each request sends:

  * Current user message
  * Previous messages (`history`)
* The backend injects history into the LLM prompt
* Enables **context-aware, multi-turn conversations**

âœ” Stateless backend
âœ” Scalable & cloud-friendly design

---

## ğŸ–¥ï¸ Getting Started (Local Setup)

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/riteshkr7578/Talk-Fusion.git
cd Talk-Fusion
```

---

### 2ï¸âƒ£ Backend Setup

```bash
cd backend
npm install
```

Create a `.env` file:

```
GROQ_API_KEY=your_groq_api_key
```

Start backend server:

```bash
npm start
```

Backend runs on:

```
http://localhost:8000
```

---

### 3ï¸âƒ£ Frontend Setup

```bash
cd ../frontend
npm install
npm run dev
```

---

### 4ï¸âƒ£ Configure Backend URL (Frontend)

```js
const API_URL = "http://localhost:8000/chat";
```

---

## ğŸ“¸ UI Highlights

* Clean chat bubbles for **User & AI**
* Markdown-formatted AI responses
* Smooth scrolling and input handling
* Minimal, distraction-free interface

---

## ğŸš€ Deployment

* **Backend**: Render (Express.js)
* **Frontend**: Vercel / Netlify
* Environment variables handled securely via platform settings

---

## ğŸ“ˆ Future Enhancements

* ğŸ” JWT-based user authentication
* ğŸ’¾ Persistent chat history
* â³ Streaming responses (typing effect)
* ğŸ¨ Dark / Light mode
* ğŸ“± Enhanced mobile responsiveness

---

## ğŸ§‘â€ğŸ’» Author

**Ritesh Kumar**
Frontend / Full-Stack Developer

Passionate about building scalable web applications and AI-powered systems.

---

## â­ Why This Project Matters

This project demonstrates:

* Modern React architecture
* Clean Express.js backend design
* AI integration using LLM APIs
* Prompt engineering awareness
* Real-world full-stack system design

Ideal for **Frontend**, **Full-Stack**, and **AI-Integrated Web Developer** roles.

---