ğŸ¤– Talk Fusion â€” Full-Stack AI Chat Application

Talk Fusion is a modern full-stack AI chat application built with React and Express.js, powered by Groqâ€™s LLaMA 3.1 model.
It delivers context-aware, multi-turn conversations, Markdown-formatted responses, and a responsive, production-grade UI inspired by real-world chat systems.

â¸»

ğŸš€ Features
	â€¢	ğŸ’¬ Real-Time AI Chat with low-latency responses
	â€¢	ğŸ§  Conversation Memory for coherent multi-turn interactions
	â€¢	âœï¸ Markdown Rendering (headings, lists, code blocks)
	â€¢	âš¡ Fast & Responsive UI (desktop + mobile)
	â€¢	ğŸ§© Intent-Aware Prompt Handling (code vs explanation)
	â€¢	ğŸ”— REST-based Frontendâ€“Backend Architecture
	â€¢	ğŸŒ CORS-enabled Express Backend
	â€¢	ğŸ—‚ï¸ Multiple Chat Sessions with sidebar navigation

â¸»

ğŸ› ï¸ Tech Stack

Frontend
	â€¢	React (Vite)
	â€¢	JavaScript (ES6+)
	â€¢	Tailwind CSS
	â€¢	React Hooks (useState, useEffect, useMemo)
	â€¢	Axios (API communication)

Backend
	â€¢	Node.js
	â€¢	Express.js
	â€¢	Groq SDK (LLaMA 3.1)
	â€¢	RESTful APIs
	â€¢	dotenv (environment configuration)
	â€¢	CORS middleware

Deployment
	â€¢	Backend: Render
	â€¢	Frontend: Vercel / Netlify

â¸»

ğŸ”— API Overview

POST /chat

Request

{
  "message": "Your message here",
  "history": [
    { "role": "user", "content": "Hello" },
    { "role": "assistant", "content": "Hi!" }
  ]
}

Response

{
  "reply": "AI-generated response"
}


â¸»

ğŸ§  Conversation Memory Design
	â€¢	The frontend maintains chat history
	â€¢	Each request sends:
	â€¢	Current user input
	â€¢	Previous conversation messages
	â€¢	The backend injects this history into the LLM prompt
	â€¢	Enables context-aware, multi-turn conversations

Design Benefits
	â€¢	Stateless backend
	â€¢	Scalable and cloud-friendly
	â€¢	Easy to extend with persistence or streaming

â¸»

ğŸ–¥ï¸ Local Setup

1ï¸âƒ£ Clone Repository

git clone https://github.com/riteshkr7578/Talk-Fusion.git
cd Talk-Fusion


â¸»

2ï¸âƒ£ Backend Setup

cd backend
npm install

Create .env:

GROQ_API_KEY=your_groq_api_key

Start server:

npm start

Backend runs at:

http://localhost:8000


â¸»

3ï¸âƒ£ Frontend Setup

cd ../frontend
npm install
npm run dev


â¸»

4ï¸âƒ£ Environment Configuration (Frontend)

const API_URL = "http://localhost:8000";


â¸»

ğŸ“¸ UI Highlights
	â€¢	Clean chat bubbles for user and AI
	â€¢	Markdown-formatted AI responses
	â€¢	Smooth auto-scrolling and input handling
	â€¢	Sidebar-based multi-chat navigation
	â€¢	Minimal, distraction-free design

â¸»

ğŸš€ Deployment Notes
	â€¢	Backend deployed on Render
	â€¢	Frontend deployed on Vercel / Netlify
	â€¢	Environment variables managed via platform settings
	â€¢	Production-ready CORS and API configuration

â¸»

ğŸ“ˆ Planned Enhancements
	â€¢	ğŸ” JWT-based authentication
	â€¢	ğŸ’¾ Persistent chat storage
	â€¢	â³ Streaming AI responses (typing effect)
	â€¢	ğŸ¨ Theme toggle (dark / light)
	â€¢	ğŸ“± Advanced mobile interactions

â¸»

ğŸ§‘â€ğŸ’» Author

Ritesh Kumar
Frontend / Full-Stack Developer

Passionate about building scalable web applications and AI-powered user experiences.

â¸»

â­ Why This Project Matters

This project showcases:
	â€¢	Real-world React application architecture
	â€¢	Clean and scalable Express.js backend
	â€¢	Practical LLM integration
	â€¢	Prompt-engineering awareness
	â€¢	End-to-end full-stack system design

